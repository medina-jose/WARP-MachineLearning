{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  1.3.0\n",
      "Keras version 2.1.2\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "print(\"Keras version\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Reshape, Lambda\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "import keras.callbacks\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', '\\x0b', '\\x0c', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "letters = sorted(list(set(Counter(string.printable).keys())))\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lambda funtions are used as a functional programming technique\n",
    "# returns an undelimited string of labels that were inputted\n",
    "def records_to_text(records):\n",
    "    artists = [record.artist for record in records]\n",
    "    albums = [record.album for record in records]   \n",
    "    artist = ''.join(list(map(lambda x: letters[int(x)], artists)))\n",
    "    album = ''.join(list(map(lambda x: letters[int(x)], albums)))\n",
    "    return artist, album\n",
    "\n",
    "def text_to_record(artist, album):\n",
    "    artist_list = list(map(lambda x: letters.index(x), artist))\n",
    "    album_list = list(map(lambda x: letters.index(x), album))\n",
    "    return Record(artist, album)\n",
    "\n",
    "def text_to_label(text, length):\n",
    "    label = list(map(lambda x: letters.index(x), text))\n",
    "    for i in range(len(label), 100):\n",
    "        label.append(5)\n",
    "    print(label)\n",
    "    return label\n",
    "\n",
    "def labels_to_text(labels):\n",
    "    return ''.join(list(map(lambda x: letters[int(x)], labels)))\n",
    "\n",
    "def is_valid_string(s):\n",
    "    for ch in s:\n",
    "        if not ch in letters:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "class TextImageGenerator:\n",
    "    \n",
    "    def __init__(self, dirpath, img_width, img_height, batch_size, downsample_factor):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        \n",
    "        # batch size determines the # of samples that go through the network per epoch\n",
    "        self.batch_size = batch_size \n",
    "        self.downsample_factor = downsample_factor\n",
    "        \n",
    "        img_dirpath = dirpath + 'img/'\n",
    "        json_dirpath = dirpath + 'json/'\n",
    "        \n",
    "        self.samples = []\n",
    "        for filename in os.listdir(img_dirpath):\n",
    "            # separate the file extension from the filename\n",
    "            name, extension = os.path.splitext(filename)\n",
    "            img_filepath = img_dirpath + filename\n",
    "            json_filepath = json_dirpath + (name + '.json')\n",
    "            \n",
    "            # obtain correct answers for each sample\n",
    "            artist = json.load(open(json_filepath, 'r'))['artist']\n",
    "            album = json.load(open(json_filepath, 'r'))['album']\n",
    "            self.samples.append([img_filepath, artist + '\\n' + album])\n",
    "        \n",
    "        # establish the number of samples\n",
    "        # make the current sample 0 (the first sample)\n",
    "        self.n = len(self.samples)\n",
    "        self.indexes = list(range(self.n))\n",
    "        self.current_index = 0\n",
    "        \n",
    "    def build_data(self):\n",
    "        # create an array for every image in the dataset\n",
    "        self.imgs = np.zeros((self.n, self.img_height, self.img_width))\n",
    "        self.records = []\n",
    "        \n",
    "        # add counter to list of samples\n",
    "        # for each sample store the image and record that corresponds to it\n",
    "        for i, (img_filepath, record) in enumerate(self.samples):\n",
    "            # print('Getting data for sample at ', img_filepath)\n",
    "            img = cv2.imread(img_filepath)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, (self.img_width, self.img_height))\n",
    "            img = img.astype(np.float32)\n",
    "            # normalize sample\n",
    "            img /= 255\n",
    "            \n",
    "            # width and height are backwards from typical Keras convention\n",
    "            # because width is the time dimension when it gets fed into the RNN\n",
    "            self.imgs[i, :, :] = img\n",
    "            self.records.append(record)\n",
    "            \n",
    "    def get_output_size(self):\n",
    "        return len(letters) + 1\n",
    "    \n",
    "    def next_sample(self):\n",
    "        self.current_index += 1\n",
    "        if self.current_index >= self.n:\n",
    "            self.current_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.imgs[self.indexes[self.current_index]], self.records[self.indexes[self.current_index]]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        # width and height are backwards from typical Keras convention\n",
    "        # because width is the time dimension when it gets fed into the RNN\n",
    "        while True:\n",
    "            # there is only one channel because sample is grayscale\n",
    "            # batch_size is the number of samples to a batch\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                X_data = np.ones([self.batch_size, 1, self.img_width, self.img_height, 1])\n",
    "            else:\n",
    "                X_data = np.ones([self.batch_size, self.img_width, self.img_height, 1])\n",
    "            \n",
    "            # the 200 here is hardcoded and might need to be played with\n",
    "            Y_data = np.ones([self.batch_size, 100])\n",
    "            input_length = np.ones((self.batch_size, 1)) * (self.img_width // self.downsample_factor - 2)\n",
    "            artist_length = np.zeros((self.batch_size, 1))\n",
    "            album_length = np.zeros((self.batch_size, 1))\n",
    "            label_length = np.zeros((self.batch_size, 1))\n",
    "            source_artist_string = []\n",
    "            source_album_string = []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                img, record = self.next_sample()\n",
    "                img = img.T\n",
    "                if K.image_data_format() == 'channels_first':\n",
    "                    img = np.expand_dims(img, 0)\n",
    "                else:\n",
    "                    img = np.expand_dims(img, -1)\n",
    "                    \n",
    "                X_data[i] = img\n",
    "                Y_data[i] = text_to_label(record.artist + ',' + record.album, 100)\n",
    "                source_artist_string.append(record.artist)\n",
    "                source_album_string.append(record.album)\n",
    "                artist_length[i] = len(record.artist)\n",
    "                album_length[i] = len(record.album)\n",
    "                label_length[i] = len(record.artist + ',' + record.album)\n",
    "                \n",
    "                inputs = {\n",
    "                         'input': X_data,\n",
    "                         'labels': Y_data,\n",
    "                         'input_length': input_length,\n",
    "                         'artist_length': artist_length,\n",
    "                         'album_length' : album_length,\n",
    "                         'label_length' : label_length,\n",
    "                         'source_artist_string': source_artist_string,\n",
    "                         'source_album_string' : source_album_string\n",
    "                         }\n",
    "                outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "                yield(inputs, outputs)\n",
    "                \n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TextImageGenerator('dataset/train/', 256, 192, 8, 4)\n",
    "model.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'artist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6024d4d959ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Text Generator output (data which will be fed into convolutional neural network)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1. the_input (image)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b27d0f1e7d6b>\u001b[0m in \u001b[0;36mnext_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[0mX_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                 \u001b[0mY_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m','\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malbum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m                 \u001b[0msource_artist_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[0msource_album_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malbum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'artist'"
     ]
    }
   ],
   "source": [
    "for inp, out in model.next_batch():\n",
    "    print('Text Generator output (data which will be fed into convolutional neural network)')\n",
    "    print('1. the_input (image)')\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        img = inp['input'][0,0,:,:]\n",
    "    else:\n",
    "        img = inp['input'][0,:,:,0]\n",
    "        \n",
    "    plt.imshow(img.T, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    print('2. the_labels (artist, album): %s is encoded as %s' % \n",
    "          (labels_to_text(inp['records'][0]), list(map(int, inp['records'][0]))))\n",
    "    print('3. input_length (width of image that is fed to the loss function): %d == %d / 4 - 2' % \n",
    "          (inp['input_length'][0], model.img_width))\n",
    "    print('4. artist_length (length of artist name): %d' % inp['artist_length'][0])\n",
    "    print('5. album_length (length of album name): %d' % inp['album_length'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Architecture With Loss & Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix this to work with records\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    \n",
    "    # the 2 is critical here since the first couple outputs of the RNN are garabage\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    print(y_pred)\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def train(img_width, load=False):\n",
    "    # training parameters\n",
    "    img_height = 192\n",
    "    \n",
    "    # network parameters\n",
    "    conv_filters = 16\n",
    "    kernel_size = (3, 3)\n",
    "    pool_size = 2\n",
    "    time_dense_size = 32\n",
    "    rnn_size = 512\n",
    "    batch_size = 32\n",
    "    downsample_factor = pool_size ** 2\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 1)\n",
    "    \n",
    "    model_train = TextImageGenerator('dataset/train/', img_width, img_height, batch_size, downsample_factor)\n",
    "    model_train.build_data()\n",
    "    model_val = TextImageGenerator('dataset/train/', img_width, img_height, batch_size, downsample_factor)\n",
    "    model_val.build_data()\n",
    "    \n",
    "    # CNN feature extraction (preprocessing)\n",
    "    act = 'relu'\n",
    "    \n",
    "    # keras.layers.Input is used to create a Keras tensor\n",
    "    print(input_shape)\n",
    "    input_data = Input(input_shape, name='input', dtype='float32')\n",
    "    print('Input: %s' % (input_data))\n",
    "    # conv_filters: # of outputs from the convolutional layer\n",
    "    # kernel_size: size of convolution window\n",
    "    # activation: activation function used. \n",
    "    inner = Conv2D(conv_filters, kernel_size, padding='same', activation=act, \n",
    "                   kernel_initializer='he_normal', name='conv1')(input_data)\n",
    "    print('Conv1 Output: %s' % (inner))\n",
    "    # pool_size: factors by which to downscale\n",
    "    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n",
    "    print('Max1 Output: %s' % (inner))\n",
    "    inner = Conv2D(conv_filters, kernel_size, padding='same', activation=act, \n",
    "                   kernel_initializer='he_normal', name='conv2')(inner)\n",
    "    print('Conv2 Output: %s' % (inner))\n",
    "    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n",
    "    print('Max2 Output: %s' % (inner))\n",
    "    # reshape & cut down tensor for input into RNN\n",
    "    conv_to_rnn_dims = (img_width // (pool_size ** 2), (img_height // (pool_size ** 2)) * conv_filters)\n",
    "    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
    "    print('Reshape Output: %s' % (inner))\n",
    "    print(\"Time dense size: %s\" % (time_dense_size))\n",
    "    inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n",
    "    print('Dense1 Output: %s' % (inner))\n",
    "    \n",
    "    # two layers of bidirectional GRUs\n",
    "    # gated recurrent unit is a variant of long short-term memory (LSTM)\n",
    "    # LSTM units/blocks are responsible for 'remembering' values in an RNN\n",
    "    gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner)\n",
    "    gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner)\n",
    "    gru1_merged = add([gru_1, gru_1b])\n",
    "    gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
    "    gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
    "    gru2_concat = concatenate([gru_2, gru_2b])\n",
    "    \n",
    "    # transform RNN output to character activations\n",
    "    inner = Dense(model_train.get_output_size(), kernel_initializer='he_normal', name='dense2')(gru2_concat)\n",
    "    y_pred = Activation('softmax', name='softmax')(inner)\n",
    "    \n",
    "    # prints a summary representation of model\n",
    "    Model(inputs=input_data, outputs=y_pred).summary()\n",
    "    \n",
    "    labels = Input(name='labels', shape=[100], dtype='float64')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    artist_length = Input(name='artist_length', shape=[1], dtype='int64')\n",
    "    album_length = Input(name='album_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    \n",
    "    # keras does not currently support loss functions with extra parameters\n",
    "    # so CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "   \n",
    "    print('Loading Model')\n",
    "    \n",
    "    # clipnorm seems to speed up convergence\n",
    "    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    \n",
    "    if load:\n",
    "        model = load_model('./tmp_model.h5', compile=False)\n",
    "    else:\n",
    "        \n",
    "        model = Model(inputs=[input_data, labels, input_length, artist_length, album_length,label_length], outputs=loss_out)\n",
    "        \n",
    "    # loss calculation occurs elsewhere, so use a dummy lambda function for loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "    \n",
    "    if not load:\n",
    "        # captures output of softmax so we can decode the output during the visualization\n",
    "        test_func = K.function([input_data], [y_pred])\n",
    "        \n",
    "        model.fit_generator(generator=model_train.next_batch(),\n",
    "                        steps_per_epoch=model_train.n,\n",
    "                        epochs=1)\n",
    "        \n",
    "#         model.fit_generator(generator=model_train.next_batch(),\n",
    "#                            steps_per_epoch=model_train.n,\n",
    "#                            epochs=1,\n",
    "#                            validation_data = model_val.next_batch(),\n",
    "#                            validation_steps = model_val.n)\n",
    "\n",
    "       # model.fit_generator(generator=model_train.next_batch(), steps_per_epoch=model_train.n, epochs=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(256, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decode_batch(out):\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = ''\n",
    "        for c in out_best:\n",
    "            if c < len(letters):\n",
    "                outstr += letters[c]\n",
    "        ret.append(outstr)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test = TextImageGenerator('dataset/test/', 256, 192, 8, 4)\n",
    "model_test.build_data()\n",
    "\n",
    "net_inp = model.get_layer(name='input').input\n",
    "net_out = model.get_layer(name='softmax').output\n",
    "\n",
    "print(net_inp)\n",
    "print(net_out)\n",
    "\n",
    "for inp_value, _ in model_test.next_batch():\n",
    "    bs = inp_value['input'].shape[0]\n",
    "    X_data = inp_value['input']\n",
    "    print(X_data)\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    net_out_value = sess.run(net_out, feed_dict={net_inp:X_data})\n",
    "    pred_texts = decode_batch(net_out_value)\n",
    "    labels = inp_value['labels']\n",
    "    for label in labels:\n",
    "        text = ''.join(list(map(lambda x: letters[int(x)], label)))\n",
    "        texts.append(text)\n",
    "    \n",
    "    for i in range(bs):\n",
    "        print('Predicted: %s\\nTrue: %s' % (pred_texts[i], texts[i]))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
